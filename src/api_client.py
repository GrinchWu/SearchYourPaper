import arxiv
from github import Github
from datetime import datetime, timedelta
from typing import Optional
import re
import base64
import requests

def extract_images_from_readme(readme_content: str, repo_name: str, token: str = None) -> list:
    """ä»READMEä¸­æå–å›¾ç‰‡URLå¹¶ä¸‹è½½ä¸ºbase64"""
    images = []
    # åŒ¹é… Markdown å›¾ç‰‡: ![alt](url) å’Œ HTML img: <img src="url">
    patterns = [
        r'!\[([^\]]*)\]\(([^)]+)\)',  # Markdown
        r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>',  # HTML
    ]

    urls = []
    for pattern in patterns:
        matches = re.findall(pattern, readme_content)
        for m in matches:
            url = m[1] if isinstance(m, tuple) and len(m) > 1 else m
            if url and not url.startswith('http'):
                # ç›¸å¯¹è·¯å¾„è½¬ç»å¯¹è·¯å¾„
                url = f"https://raw.githubusercontent.com/{repo_name}/main/{url}"
            if url:
                urls.append(url)

    # ä¸‹è½½å›¾ç‰‡ï¼ˆé™åˆ¶æ•°é‡å’Œå¤§å°ï¼‰
    headers = {"Authorization": f"token {token}"} if token else {}
    for url in urls[:5]:  # æœ€å¤š5å¼ å›¾
        try:
            if any(ext in url.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg']):
                resp = requests.get(url, headers=headers, timeout=10)
                if resp.status_code == 200 and len(resp.content) < 5 * 1024 * 1024:  # <5MB
                    b64 = base64.b64encode(resp.content).decode()
                    ext = url.split('.')[-1].split('?')[0].lower()
                    mime = {'png': 'image/png', 'jpg': 'image/jpeg', 'jpeg': 'image/jpeg',
                            'gif': 'image/gif', 'webp': 'image/webp', 'svg': 'image/svg+xml'}.get(ext, 'image/png')
                    images.append({"url": f"data:{mime};base64,{b64}", "source_url": url})
        except:
            pass
    return images

def extract_images_from_pdf(pdf_url: str, max_images: int = 5) -> list:
    """ä»PDFä¸­æå–å›¾ç‰‡ï¼ˆéœ€è¦PyMuPDFï¼‰"""
    images = []
    try:
        import fitz  # PyMuPDF

        # ä¸‹è½½PDF
        resp = requests.get(pdf_url, timeout=30)
        if resp.status_code != 200:
            return images

        doc = fitz.open(stream=resp.content, filetype="pdf")
        img_count = 0

        # ä¼˜å…ˆæå–å‰å‡ é¡µçš„å¤§å›¾ï¼ˆé€šå¸¸æ˜¯æ¶æ„å›¾ã€æµç¨‹å›¾ï¼‰
        for page_num in range(min(len(doc), 15)):  # æ‰«æå‰15é¡µ
            page = doc[page_num]

            # æ–¹æ³•1: æå–åµŒå…¥å›¾ç‰‡
            image_list = page.get_images()
            for img_index, img in enumerate(image_list):
                if img_count >= max_images:
                    break
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    image_ext = base_image["ext"]

                    # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå›¾ç‰‡ï¼ˆ>5KBï¼‰
                    if len(image_bytes) < 5000:
                        continue

                    b64 = base64.b64encode(image_bytes).decode()
                    mime = f"image/{image_ext}"
                    images.append({
                        "url": f"data:{mime};base64,{b64}",
                        "page": page_num + 1,
                        "description": f"Page {page_num + 1}, Image {img_index + 1}"
                    })
                    img_count += 1
                except:
                    pass

            # æ–¹æ³•2: å¦‚æœåµŒå…¥å›¾ç‰‡ä¸å¤Ÿï¼Œæ¸²æŸ“é¡µé¢ä¸ºå›¾ç‰‡ï¼ˆæ•è·å›¾è¡¨ï¼‰
            if img_count < max_images and page_num < 5:
                try:
                    # æ¸²æŸ“é¡µé¢ä¸ºå›¾ç‰‡
                    mat = fitz.Matrix(2, 2)  # 2xç¼©æ”¾
                    pix = page.get_pixmap(matrix=mat)
                    if pix.width > 200 and pix.height > 200:
                        img_bytes = pix.tobytes("png")
                        if len(img_bytes) > 10000:  # >10KB
                            b64 = base64.b64encode(img_bytes).decode()
                            images.append({
                                "url": f"data:image/png;base64,{b64}",
                                "page": page_num + 1,
                                "description": f"Page {page_num + 1} (rendered)"
                            })
                            img_count += 1
                except:
                    pass

            if img_count >= max_images:
                break
        doc.close()
    except ImportError:
        pass  # PyMuPDF æœªå®‰è£…
    except:
        pass
    return images

def get_date_range(period: str) -> tuple[datetime, datetime]:
    """æ ¹æ®æ—¶é—´æ®µè¿”å›æ—¥æœŸèŒƒå›´"""
    end = datetime.now()
    periods = {
        "yesterday": 1, "past_week": 7, "past_month": 30,
        "past_3months": 90, "past_year": 365
    }
    days = periods.get(period, 7)
    return end - timedelta(days=days), end

def search_arxiv(query: str, start_date: datetime, end_date: datetime, max_results: int = 20) -> list[dict]:
    """æœç´¢arXivè®ºæ–‡ï¼Œæ”¯æŒåˆ†æ‰¹æœç´¢"""
    client = arxiv.Client()
    results = []
    batch_size = 50
    offset = 0

    while len(results) < max_results:
        remaining = max_results - len(results)
        current_batch = min(batch_size, remaining)
        search = arxiv.Search(query=query, max_results=current_batch, sort_by=arxiv.SortCriterion.SubmittedDate)
        search.start = offset

        batch_results = []
        for paper in client.results(search):
            if start_date <= paper.published.replace(tzinfo=None) <= end_date:
                batch_results.append({
                    "title": paper.title, "authors": [a.name for a in paper.authors],
                    "abstract": paper.summary, "url": paper.entry_id,
                    "pdf_url": paper.pdf_url, "published": paper.published.strftime("%Y-%m-%d"),
                    "categories": paper.categories, "source": "arxiv"
                })

        if not batch_results:
            break
        results.extend(batch_results)
        offset += current_batch
        if len(batch_results) < current_batch:
            break

    return results[:max_results]

def search_github(query: str, start_date: datetime, end_date: datetime, token: Optional[str] = None, max_results: int = 20) -> list[dict]:
    """æœç´¢GitHubé¡¹ç›®ï¼Œæ”¯æŒåˆ†æ‰¹æœç´¢"""
    g = Github(token) if token else Github()
    date_query = f"{query} pushed:{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}"
    results = []
    batch_size = 30
    page = 0

    while len(results) < max_results:
        remaining = max_results - len(results)
        repos = g.search_repositories(date_query, sort="stars", order="desc")
        batch = list(repos.get_page(page))
        if not batch:
            break
        for repo in batch[:remaining]:
            results.append({
                "title": repo.full_name, "description": repo.description or "",
                "url": repo.html_url, "stars": repo.stargazers_count,
                "language": repo.language, "updated": repo.updated_at.strftime("%Y-%m-%d"),
                "topics": repo.get_topics(), "source": "github"
            })
            if len(results) >= max_results:
                break
        page += 1
        if len(batch) < batch_size:
            break

    return results

def search_trending(query: str, token: Optional[str] = None, max_results: int = 20, search_new: bool = True) -> list[dict]:
    """æœç´¢çƒ­é—¨é¡¹ç›®

    Args:
        query: æœç´¢å…³é”®è¯
        token: GitHub token
        max_results: æœ€å¤§ç»“æœæ•°
        search_new: True=æœç´¢æ–°åˆ›å»ºçš„é¡¹ç›®(created:), False=æœç´¢æœ‰æ›´æ–°çš„é¡¹ç›®(pushed:)
    """
    end_date = datetime.now()
    start_date = end_date - timedelta(days=3)
    results = []

    # æœç´¢GitHubçƒ­é—¨é¡¹ç›®
    g = Github(token) if token else Github()
    date_param = "created" if search_new else "pushed"
    date_query = f"{query} {date_param}:{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}"
    repos = g.search_repositories(date_query, sort="stars", order="desc")
    for repo in repos[:max_results]:
        results.append({
            "title": repo.full_name, "description": repo.description or "",
            "url": repo.html_url, "stars": repo.stargazers_count,
            "language": repo.language, "updated": repo.updated_at.strftime("%Y-%m-%d"),
            "created": repo.created_at.strftime("%Y-%m-%d"),
            "topics": repo.get_topics(), "source": "github"
        })

    # æŒ‰çƒ­åº¦æ’åº
    results.sort(key=lambda x: x.get('stars', 0), reverse=True)
    return results

def get_repo_content(repo_name: str, token: Optional[str] = None, fetch_images: bool = False) -> dict:
    """è·å–GitHubä»“åº“çš„è¯¦ç»†å†…å®¹ç”¨äºæ·±åº¦åˆ†æ"""
    g = Github(token) if token else Github()
    repo = g.get_repo(repo_name)
    content = {"readme": "", "structure": [], "key_files": [], "images": []}

    # è·å–README
    for name in ["README.md", "README.rst", "README.txt", "README"]:
        try:
            readme = repo.get_contents(name)
            content["readme"] = base64.b64decode(readme.content).decode('utf-8', errors='ignore')
            break
        except: pass

    # è·å–é¡¹ç›®ç»“æ„å’Œå…³é”®æ–‡ä»¶
    code_exts = {'.py', '.js', '.ts', '.java', '.go', '.rs', '.cpp', '.c', '.h'}
    doc_exts = {'.md', '.rst', '.txt'}

    def scan_dir(path="", depth=0):
        if depth > 2: return
        try:
            items = repo.get_contents(path)
            for item in items:
                if item.type == "dir" and not item.name.startswith('.'):
                    content["structure"].append(f"{'  '*depth}ğŸ“ {item.name}/")
                    scan_dir(item.path, depth + 1)
                elif item.type == "file":
                    ext = '.' + item.name.split('.')[-1] if '.' in item.name else ''
                    content["structure"].append(f"{'  '*depth}ğŸ“„ {item.name}")
                    # è¯»å–å…³é”®æ–‡ä»¶ï¼ˆé™åˆ¶å¤§å°å’Œæ•°é‡ï¼‰
                    if len(content["key_files"]) < 10 and item.size < 50000:
                        if ext in code_exts or ext in doc_exts or item.name in ['setup.py', 'requirements.txt', 'package.json']:
                            try:
                                file_content = base64.b64decode(repo.get_contents(item.path).content).decode('utf-8', errors='ignore')
                                content["key_files"].append({"name": item.path, "content": file_content[:8000]})
                            except: pass
        except: pass

    scan_dir()

    # æå–å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if fetch_images and content["readme"]:
        content["images"] = extract_images_from_readme(content["readme"], repo_name, token)

    return content

# ==================== Hugging Face æœç´¢ ====================

def search_huggingface(query: str, start_date: datetime = None, end_date: datetime = None,
                       max_results: int = 20, search_type: str = "models") -> list[dict]:
    """æœç´¢ Hugging Face æ¨¡å‹æˆ–æ•°æ®é›†

    Args:
        query: æœç´¢å…³é”®è¯
        start_date: å¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
        max_results: æœ€å¤§ç»“æœæ•°
        search_type: "models" æˆ– "datasets"
    """
    results = []
    try:
        from huggingface_hub import HfApi
        api = HfApi()

        if search_type == "models":
            items = api.list_models(
                search=query,
                sort="lastModified",
                direction=-1,
                limit=max_results
            )
        else:
            items = api.list_datasets(
                search=query,
                sort="lastModified",
                direction=-1,
                limit=max_results
            )

        for item in items:
            # æ—¶é—´è¿‡æ»¤
            if hasattr(item, 'lastModified') and item.lastModified:
                modified = item.lastModified.replace(tzinfo=None) if hasattr(item.lastModified, 'replace') else None
                if modified and start_date and end_date:
                    if not (start_date <= modified <= end_date):
                        continue

            results.append({
                "title": item.id,
                "description": getattr(item, 'description', '') or '',
                "url": f"https://huggingface.co/{item.id}",
                "downloads": getattr(item, 'downloads', 0) or 0,
                "likes": getattr(item, 'likes', 0) or 0,
                "updated": item.lastModified.strftime("%Y-%m-%d") if hasattr(item, 'lastModified') and item.lastModified else "",
                "tags": list(getattr(item, 'tags', []) or [])[:5],
                "source": "huggingface",
                "type": search_type
            })

            if len(results) >= max_results:
                break

    except ImportError:
        pass  # huggingface_hub æœªå®‰è£…
    except Exception as e:
        print(f"Hugging Face search error: {e}")

    return results

# ==================== ModelScope æœç´¢ ====================

def search_modelscope(query: str, start_date: datetime = None, end_date: datetime = None,
                      max_results: int = 20) -> list[dict]:
    """æœç´¢ ModelScope æ¨¡å‹

    Args:
        query: æœç´¢å…³é”®è¯
        start_date: å¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
        max_results: æœ€å¤§ç»“æœæ•°
    """
    results = []
    try:
        # ä½¿ç”¨ ModelScope APIï¼ˆRESTæ–¹å¼ï¼Œä¸éœ€è¦å®‰è£…SDKï¼‰
        url = "https://modelscope.cn/api/v1/models"
        params = {
            "Query": query,
            "PageSize": max_results,
            "SortBy": "gmt_modified"
        }
        resp = requests.get(url, params=params, timeout=30)
        if resp.status_code == 200:
            data = resp.json()
            models = data.get("Data", {}).get("Models", [])

            for model in models:
                results.append({
                    "title": model.get("Name", ""),
                    "description": model.get("ChineseDescription", "") or model.get("Description", ""),
                    "url": f"https://modelscope.cn/models/{model.get('Path', '')}",
                    "downloads": model.get("Downloads", 0),
                    "likes": model.get("Likes", 0),
                    "updated": model.get("LastUpdatedTime", "")[:10] if model.get("LastUpdatedTime") else "",
                    "tags": model.get("Tags", [])[:5] if model.get("Tags") else [],
                    "source": "modelscope",
                    "type": "models"
                })

                if len(results) >= max_results:
                    break

    except Exception as e:
        print(f"ModelScope search error: {e}")

    return results

def get_huggingface_content(model_id: str) -> dict:
    """è·å– Hugging Face æ¨¡å‹çš„è¯¦ç»†å†…å®¹ç”¨äºæ·±åº¦åˆ†æ"""
    content = {"readme": "", "model_info": "", "files": []}
    try:
        from huggingface_hub import HfApi, hf_hub_download
        api = HfApi()

        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = api.model_info(model_id)
        content["model_info"] = f"""
æ¨¡å‹ID: {model_id}
ä½œè€…: {getattr(model_info, 'author', '')}
ä¸‹è½½é‡: {getattr(model_info, 'downloads', 0)}
ç‚¹èµæ•°: {getattr(model_info, 'likes', 0)}
æ ‡ç­¾: {', '.join(getattr(model_info, 'tags', []) or [])}
Pipeline: {getattr(model_info, 'pipeline_tag', '')}
åº“: {getattr(model_info, 'library_name', '')}
"""

        # è·å– README
        try:
            readme_path = hf_hub_download(repo_id=model_id, filename="README.md")
            with open(readme_path, 'r', encoding='utf-8', errors='ignore') as f:
                content["readme"] = f.read()[:15000]
        except:
            pass

        # è·å–æ–‡ä»¶åˆ—è¡¨
        try:
            files = api.list_repo_files(model_id)
            content["files"] = files[:50]
        except:
            pass

    except Exception as e:
        print(f"Get HuggingFace content error: {e}")

    return content

def get_modelscope_content(model_path: str) -> dict:
    """è·å– ModelScope æ¨¡å‹çš„è¯¦ç»†å†…å®¹ç”¨äºæ·±åº¦åˆ†æ"""
    content = {"readme": "", "model_info": "", "files": []}
    try:
        # è·å–æ¨¡å‹è¯¦æƒ…
        url = f"https://modelscope.cn/api/v1/models/{model_path}"
        resp = requests.get(url, timeout=30)
        if resp.status_code == 200:
            data = resp.json().get("Data", {})
            content["model_info"] = f"""
æ¨¡å‹åç§°: {data.get('Name', '')}
æè¿°: {data.get('ChineseDescription', '') or data.get('Description', '')}
ä¸‹è½½é‡: {data.get('Downloads', 0)}
æ ‡ç­¾: {', '.join(data.get('Tags', []) or [])}
ä»»åŠ¡: {data.get('Task', '')}
"""
            content["readme"] = data.get("ReadmeContent", "")[:15000]

        # è·å–æ–‡ä»¶åˆ—è¡¨
        files_url = f"https://modelscope.cn/api/v1/models/{model_path}/repo/files"
        files_resp = requests.get(files_url, timeout=30)
        if files_resp.status_code == 200:
            files_data = files_resp.json().get("Data", {}).get("Files", [])
            content["files"] = [f.get("Name", "") for f in files_data[:50]]

    except Exception as e:
        print(f"Get ModelScope content error: {e}")

    return content
